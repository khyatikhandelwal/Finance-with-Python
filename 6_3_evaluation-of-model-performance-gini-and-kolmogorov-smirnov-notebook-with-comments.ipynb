{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data and Selecting the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_train = pd.read_csv('loan_data_inputs_train.csv', index_col = 0)\n",
    "loan_data_targets_train = pd.read_csv('loan_data_targets_train.csv', index_col = 0, header = None)\n",
    "loan_data_inputs_test = pd.read_csv('loan_data_inputs_test.csv', index_col = 0)\n",
    "loan_data_targets_test = pd.read_csv('loan_data_targets_test.csv', index_col = 0, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we select a limited set of input variables in a new dataframe.\n",
    "inputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n",
    "'grade:B',\n",
    "'grade:C',\n",
    "'grade:D',\n",
    "'grade:E',\n",
    "'grade:F',\n",
    "'grade:G',\n",
    "'home_ownership:RENT_OTHER_NONE_ANY',\n",
    "'home_ownership:OWN',\n",
    "'home_ownership:MORTGAGE',\n",
    "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    "'addr_state:NM_VA',\n",
    "'addr_state:NY',\n",
    "'addr_state:OK_TN_MO_LA_MD_NC',\n",
    "'addr_state:CA',\n",
    "'addr_state:UT_KY_AZ_NJ',\n",
    "'addr_state:AR_MI_PA_OH_MN',\n",
    "'addr_state:RI_MA_DE_SD_IN',\n",
    "'addr_state:GA_WA_OR',\n",
    "'addr_state:WI_MT',\n",
    "'addr_state:TX',\n",
    "'addr_state:IL_CT',\n",
    "'addr_state:KS_SC_CO_VT_AK_MS',\n",
    "'addr_state:WV_NH_WY_DC_ME_ID',\n",
    "'verification_status:Not Verified',\n",
    "'verification_status:Source Verified',\n",
    "'verification_status:Verified',\n",
    "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    "'purpose:credit_card',\n",
    "'purpose:debt_consolidation',\n",
    "'purpose:oth__med__vacation',\n",
    "'purpose:major_purch__car__home_impr',\n",
    "'initial_list_status:f',\n",
    "'initial_list_status:w',\n",
    "'term:36',\n",
    "'term:60',\n",
    "'emp_length:0',\n",
    "'emp_length:1',\n",
    "'emp_length:2-4',\n",
    "'emp_length:5-6',\n",
    "'emp_length:7-9',\n",
    "'emp_length:10',\n",
    "'mths_since_issue_d:<38',\n",
    "'mths_since_issue_d:38-39',\n",
    "'mths_since_issue_d:40-41',\n",
    "'mths_since_issue_d:42-48',\n",
    "'mths_since_issue_d:49-52',\n",
    "'mths_since_issue_d:53-64',\n",
    "'mths_since_issue_d:65-84',\n",
    "'mths_since_issue_d:>84',\n",
    "'int_rate:<9.548',\n",
    "'int_rate:9.548-12.025',\n",
    "'int_rate:12.025-15.74',\n",
    "'int_rate:15.74-20.281',\n",
    "'int_rate:>20.281',\n",
    "'mths_since_earliest_cr_line:<140',\n",
    "'mths_since_earliest_cr_line:141-164',\n",
    "'mths_since_earliest_cr_line:165-247',\n",
    "'mths_since_earliest_cr_line:248-270',\n",
    "'mths_since_earliest_cr_line:271-352',\n",
    "'mths_since_earliest_cr_line:>352',\n",
    "'delinq_2yrs:0',\n",
    "'delinq_2yrs:1-3',\n",
    "'delinq_2yrs:>=4',\n",
    "'inq_last_6mths:0',\n",
    "'inq_last_6mths:1-2',\n",
    "'inq_last_6mths:3-6',\n",
    "'inq_last_6mths:>6',\n",
    "'open_acc:0',\n",
    "'open_acc:1-3',\n",
    "'open_acc:4-12',\n",
    "'open_acc:13-17',\n",
    "'open_acc:18-22',\n",
    "'open_acc:23-25',\n",
    "'open_acc:26-30',\n",
    "'open_acc:>=31',\n",
    "'pub_rec:0-2',\n",
    "'pub_rec:3-4',\n",
    "'pub_rec:>=5',\n",
    "'total_acc:<=27',\n",
    "'total_acc:28-51',\n",
    "'total_acc:>=52',\n",
    "'acc_now_delinq:0',\n",
    "'acc_now_delinq:>=1',\n",
    "'total_rev_hi_lim:<=5K',\n",
    "'total_rev_hi_lim:5K-10K',\n",
    "'total_rev_hi_lim:10K-20K',\n",
    "'total_rev_hi_lim:20K-30K',\n",
    "'total_rev_hi_lim:30K-40K',\n",
    "'total_rev_hi_lim:40K-55K',\n",
    "'total_rev_hi_lim:55K-95K',\n",
    "'total_rev_hi_lim:>95K',\n",
    "'annual_inc:<20K',\n",
    "'annual_inc:20K-30K',\n",
    "'annual_inc:30K-40K',\n",
    "'annual_inc:40K-50K',\n",
    "'annual_inc:50K-60K',\n",
    "'annual_inc:60K-70K',\n",
    "'annual_inc:70K-80K',\n",
    "'annual_inc:80K-90K',\n",
    "'annual_inc:90K-100K',\n",
    "'annual_inc:100K-120K',\n",
    "'annual_inc:120K-140K',\n",
    "'annual_inc:>140K',\n",
    "'dti:<=1.4',\n",
    "'dti:1.4-3.5',\n",
    "'dti:3.5-7.7',\n",
    "'dti:7.7-10.5',\n",
    "'dti:10.5-16.1',\n",
    "'dti:16.1-20.3',\n",
    "'dti:20.3-21.7',\n",
    "'dti:21.7-22.4',\n",
    "'dti:22.4-35',\n",
    "'dti:>35',\n",
    "'mths_since_last_delinq:Missing',\n",
    "'mths_since_last_delinq:0-3',\n",
    "'mths_since_last_delinq:4-30',\n",
    "'mths_since_last_delinq:31-56',\n",
    "'mths_since_last_delinq:>=57',\n",
    "'mths_since_last_record:Missing',\n",
    "'mths_since_last_record:0-2',\n",
    "'mths_since_last_record:3-20',\n",
    "'mths_since_last_record:21-31',\n",
    "'mths_since_last_record:32-80',\n",
    "'mths_since_last_record:81-86',\n",
    "'mths_since_last_record:>=86',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store the names of the reference category dummy variables in a list.\n",
    "ref_categories = ['grade:G',\n",
    "'home_ownership:RENT_OTHER_NONE_ANY',\n",
    "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    "'verification_status:Verified',\n",
    "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    "'initial_list_status:f',\n",
    "'term:60',\n",
    "'emp_length:0',\n",
    "'mths_since_issue_d:>84',\n",
    "'int_rate:>20.281',\n",
    "'mths_since_earliest_cr_line:<140',\n",
    "'delinq_2yrs:>=4',\n",
    "'inq_last_6mths:>6',\n",
    "'open_acc:0',\n",
    "'pub_rec:0-2',\n",
    "'total_acc:<=27',\n",
    "'acc_now_delinq:0',\n",
    "'total_rev_hi_lim:<=5K',\n",
    "'annual_inc:<20K',\n",
    "'dti:>35',\n",
    "'mths_since_last_delinq:0-3',\n",
    "'mths_since_last_record:0-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\n",
    "# From the dataframe with input variables, we drop the variables with variable names in the list with reference categories. \n",
    "inputs_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()\n",
    "# We create an instance of an object from the 'LogisticRegression' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None\n",
    "# Sets the pandas dataframe options to display all columns/ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg.fit(inputs_train, loan_data_targets_train)\n",
    "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
    "# with inputs (independent variables) contained in the first dataframe\n",
    "# and targets (dependent variables) contained in the second dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_\n",
    "# Displays the intercept contain in the estimated (\"fitted\") object from the 'LogisticRegression' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg.coef_\n",
    "# Displays the coefficients contained in the estimated (\"fitted\") object from the 'LogisticRegression' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = inputs_train.columns.values\n",
    "# Stores the names of the columns of a dataframe in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
    "summary_table['Coefficients'] = np.transpose(reg.coef_)\n",
    "# Creates a new column in the dataframe, called 'Coefficients',\n",
    "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
    "summary_table.index = summary_table.index + 1\n",
    "# Increases the index of every row of the dataframe with 1.\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "# Assigns values of the row with index 0 of the dataframe.\n",
    "summary_table = summary_table.sort_index()\n",
    "# Sorts the dataframe by index.\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Logistic Regression Model with P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P values for sklearn logistic regression.\n",
    "\n",
    "# Class to display p-values for logistic regression in sklearn.\n",
    "\n",
    "from sklearn import linear_model\n",
    "import scipy.stats as stat\n",
    "\n",
    "class LogisticRegression_with_p_values:\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):#,**kwargs):\n",
    "        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model.fit(X,y)\n",
    "        \n",
    "        #### Get p-values for the fitted model ####\n",
    "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
    "        denom = np.tile(denom,(X.shape[1],1)).T\n",
    "        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n",
    "        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
    "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n",
    "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n",
    "        \n",
    "        self.coef_ = self.model.coef_\n",
    "        self.intercept_ = self.model.intercept_\n",
    "        #self.z_scores = z_scores\n",
    "        self.p_values = p_values\n",
    "        #self.sigma_estimates = sigma_estimates\n",
    "        #self.F_ij = F_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import scipy.stats as stat\n",
    "\n",
    "class LogisticRegression_with_p_values:\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.model = linear_model.LogisticRegression(*args,**kwargs)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model.fit(X,y)\n",
    "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
    "        denom = np.tile(denom,(X.shape[1],1)).T\n",
    "        F_ij = np.dot((X / denom).T,X)\n",
    "        Cramer_Rao = np.linalg.inv(F_ij)\n",
    "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "        z_scores = self.model.coef_[0] / sigma_estimates\n",
    "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores]\n",
    "        self.coef_ = self.model.coef_\n",
    "        self.intercept_ = self.model.intercept_\n",
    "        self.p_values = p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression_with_p_values()\n",
    "# We create an instance of an object from the newly created 'LogisticRegression_with_p_values()' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg.fit(inputs_train, loan_data_targets_train)\n",
    "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
    "# with inputs (independent variables) contained in the first dataframe\n",
    "# and targets (dependent variables) contained in the second dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above.\n",
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "summary_table['Coefficients'] = np.transpose(reg.coef_)\n",
    "summary_table.index = summary_table.index + 1\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "summary_table = summary_table.sort_index()\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list.\n",
    "p_values = reg.p_values\n",
    "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the intercept for completeness.\n",
    "p_values = np.append(np.nan, np.array(p_values))\n",
    "# We add the value 'NaN' in the beginning of the variable with p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table['p_values'] = p_values\n",
    "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to remove some features, the coefficients for all or almost all of the dummy variables for which,\n",
    "# are not tatistically significant.\n",
    "\n",
    "# We do that by specifying another list of dummy variables as reference categories, and a list of variables to remove.\n",
    "# Then, we are going to drop the two datasets from the original list of dummy variables.\n",
    "\n",
    "# Variables\n",
    "inputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n",
    "'grade:B',\n",
    "'grade:C',\n",
    "'grade:D',\n",
    "'grade:E',\n",
    "'grade:F',\n",
    "'grade:G',\n",
    "'home_ownership:RENT_OTHER_NONE_ANY',\n",
    "'home_ownership:OWN',\n",
    "'home_ownership:MORTGAGE',\n",
    "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    "'addr_state:NM_VA',\n",
    "'addr_state:NY',\n",
    "'addr_state:OK_TN_MO_LA_MD_NC',\n",
    "'addr_state:CA',\n",
    "'addr_state:UT_KY_AZ_NJ',\n",
    "'addr_state:AR_MI_PA_OH_MN',\n",
    "'addr_state:RI_MA_DE_SD_IN',\n",
    "'addr_state:GA_WA_OR',\n",
    "'addr_state:WI_MT',\n",
    "'addr_state:TX',\n",
    "'addr_state:IL_CT',\n",
    "'addr_state:KS_SC_CO_VT_AK_MS',\n",
    "'addr_state:WV_NH_WY_DC_ME_ID',\n",
    "'verification_status:Not Verified',\n",
    "'verification_status:Source Verified',\n",
    "'verification_status:Verified',\n",
    "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    "'purpose:credit_card',\n",
    "'purpose:debt_consolidation',\n",
    "'purpose:oth__med__vacation',\n",
    "'purpose:major_purch__car__home_impr',\n",
    "'initial_list_status:f',\n",
    "'initial_list_status:w',\n",
    "'term:36',\n",
    "'term:60',\n",
    "'emp_length:0',\n",
    "'emp_length:1',\n",
    "'emp_length:2-4',\n",
    "'emp_length:5-6',\n",
    "'emp_length:7-9',\n",
    "'emp_length:10',\n",
    "'mths_since_issue_d:<38',\n",
    "'mths_since_issue_d:38-39',\n",
    "'mths_since_issue_d:40-41',\n",
    "'mths_since_issue_d:42-48',\n",
    "'mths_since_issue_d:49-52',\n",
    "'mths_since_issue_d:53-64',\n",
    "'mths_since_issue_d:65-84',\n",
    "'mths_since_issue_d:>84',\n",
    "'int_rate:<9.548',\n",
    "'int_rate:9.548-12.025',\n",
    "'int_rate:12.025-15.74',\n",
    "'int_rate:15.74-20.281',\n",
    "'int_rate:>20.281',\n",
    "'mths_since_earliest_cr_line:<140',\n",
    "'mths_since_earliest_cr_line:141-164',\n",
    "'mths_since_earliest_cr_line:165-247',\n",
    "'mths_since_earliest_cr_line:248-270',\n",
    "'mths_since_earliest_cr_line:271-352',\n",
    "'mths_since_earliest_cr_line:>352',\n",
    "'inq_last_6mths:0',\n",
    "'inq_last_6mths:1-2',\n",
    "'inq_last_6mths:3-6',\n",
    "'inq_last_6mths:>6',\n",
    "'acc_now_delinq:0',\n",
    "'acc_now_delinq:>=1',\n",
    "'annual_inc:<20K',\n",
    "'annual_inc:20K-30K',\n",
    "'annual_inc:30K-40K',\n",
    "'annual_inc:40K-50K',\n",
    "'annual_inc:50K-60K',\n",
    "'annual_inc:60K-70K',\n",
    "'annual_inc:70K-80K',\n",
    "'annual_inc:80K-90K',\n",
    "'annual_inc:90K-100K',\n",
    "'annual_inc:100K-120K',\n",
    "'annual_inc:120K-140K',\n",
    "'annual_inc:>140K',\n",
    "'dti:<=1.4',\n",
    "'dti:1.4-3.5',\n",
    "'dti:3.5-7.7',\n",
    "'dti:7.7-10.5',\n",
    "'dti:10.5-16.1',\n",
    "'dti:16.1-20.3',\n",
    "'dti:20.3-21.7',\n",
    "'dti:21.7-22.4',\n",
    "'dti:22.4-35',\n",
    "'dti:>35',\n",
    "'mths_since_last_delinq:Missing',\n",
    "'mths_since_last_delinq:0-3',\n",
    "'mths_since_last_delinq:4-30',\n",
    "'mths_since_last_delinq:31-56',\n",
    "'mths_since_last_delinq:>=57',\n",
    "'mths_since_last_record:Missing',\n",
    "'mths_since_last_record:0-2',\n",
    "'mths_since_last_record:3-20',\n",
    "'mths_since_last_record:21-31',\n",
    "'mths_since_last_record:32-80',\n",
    "'mths_since_last_record:81-86',\n",
    "'mths_since_last_record:>=86',\n",
    "]]\n",
    "\n",
    "ref_categories = ['grade:G',\n",
    "'home_ownership:RENT_OTHER_NONE_ANY',\n",
    "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    "'verification_status:Verified',\n",
    "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    "'initial_list_status:f',\n",
    "'term:60',\n",
    "'emp_length:0',\n",
    "'mths_since_issue_d:>84',\n",
    "'int_rate:>20.281',\n",
    "'mths_since_earliest_cr_line:<140',\n",
    "'inq_last_6mths:>6',\n",
    "'acc_now_delinq:0',\n",
    "'annual_inc:<20K',\n",
    "'dti:>35',\n",
    "'mths_since_last_delinq:0-3',\n",
    "'mths_since_last_record:0-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\n",
    "inputs_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we run a new model.\n",
    "reg2 = LogisticRegression_with_p_values()\n",
    "reg2.fit(inputs_train, loan_data_targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = inputs_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above.\n",
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "summary_table['Coefficients'] = np.transpose(reg2.coef_)\n",
    "summary_table.index = summary_table.index + 1\n",
    "summary_table.loc[0] = ['Intercept', reg2.intercept_[0]]\n",
    "summary_table = summary_table.sort_index()\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the 'p_values' here, just as we did before.\n",
    "p_values = reg2.p_values\n",
    "p_values = np.append(np.nan,np.array(p_values))\n",
    "summary_table['p_values'] = p_values\n",
    "summary_table\n",
    "# Here we get the results for our final PD model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(reg2, open('pd_model.sav', 'wb'))\n",
    "# Here we export our model to a 'SAV' file with file name 'pd_model.sav'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD Model Validation (Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample validation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, from the dataframe with inputs for testing, we keep the same variables that we used in our final PD model.\n",
    "inputs_test_with_ref_cat = loan_data_inputs_test.loc[: , ['grade:A',\n",
    "'grade:B',\n",
    "'grade:C',\n",
    "'grade:D',\n",
    "'grade:E',\n",
    "'grade:F',\n",
    "'grade:G',\n",
    "'home_ownership:RENT_OTHER_NONE_ANY',\n",
    "'home_ownership:OWN',\n",
    "'home_ownership:MORTGAGE',\n",
    "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    "'addr_state:NM_VA',\n",
    "'addr_state:NY',\n",
    "'addr_state:OK_TN_MO_LA_MD_NC',\n",
    "'addr_state:CA',\n",
    "'addr_state:UT_KY_AZ_NJ',\n",
    "'addr_state:AR_MI_PA_OH_MN',\n",
    "'addr_state:RI_MA_DE_SD_IN',\n",
    "'addr_state:GA_WA_OR',\n",
    "'addr_state:WI_MT',\n",
    "'addr_state:TX',\n",
    "'addr_state:IL_CT',\n",
    "'addr_state:KS_SC_CO_VT_AK_MS',\n",
    "'addr_state:WV_NH_WY_DC_ME_ID',\n",
    "'verification_status:Not Verified',\n",
    "'verification_status:Source Verified',\n",
    "'verification_status:Verified',\n",
    "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    "'purpose:credit_card',\n",
    "'purpose:debt_consolidation',\n",
    "'purpose:oth__med__vacation',\n",
    "'purpose:major_purch__car__home_impr',\n",
    "'initial_list_status:f',\n",
    "'initial_list_status:w',\n",
    "'term:36',\n",
    "'term:60',\n",
    "'emp_length:0',\n",
    "'emp_length:1',\n",
    "'emp_length:2-4',\n",
    "'emp_length:5-6',\n",
    "'emp_length:7-9',\n",
    "'emp_length:10',\n",
    "'mths_since_issue_d:<38',\n",
    "'mths_since_issue_d:38-39',\n",
    "'mths_since_issue_d:40-41',\n",
    "'mths_since_issue_d:42-48',\n",
    "'mths_since_issue_d:49-52',\n",
    "'mths_since_issue_d:53-64',\n",
    "'mths_since_issue_d:65-84',\n",
    "'mths_since_issue_d:>84',\n",
    "'int_rate:<9.548',\n",
    "'int_rate:9.548-12.025',\n",
    "'int_rate:12.025-15.74',\n",
    "'int_rate:15.74-20.281',\n",
    "'int_rate:>20.281',\n",
    "'mths_since_earliest_cr_line:<140',\n",
    "'mths_since_earliest_cr_line:141-164',\n",
    "'mths_since_earliest_cr_line:165-247',\n",
    "'mths_since_earliest_cr_line:248-270',\n",
    "'mths_since_earliest_cr_line:271-352',\n",
    "'mths_since_earliest_cr_line:>352',\n",
    "'inq_last_6mths:0',\n",
    "'inq_last_6mths:1-2',\n",
    "'inq_last_6mths:3-6',\n",
    "'inq_last_6mths:>6',\n",
    "'acc_now_delinq:0',\n",
    "'acc_now_delinq:>=1',\n",
    "'annual_inc:<20K',\n",
    "'annual_inc:20K-30K',\n",
    "'annual_inc:30K-40K',\n",
    "'annual_inc:40K-50K',\n",
    "'annual_inc:50K-60K',\n",
    "'annual_inc:60K-70K',\n",
    "'annual_inc:70K-80K',\n",
    "'annual_inc:80K-90K',\n",
    "'annual_inc:90K-100K',\n",
    "'annual_inc:100K-120K',\n",
    "'annual_inc:120K-140K',\n",
    "'annual_inc:>140K',\n",
    "'dti:<=1.4',\n",
    "'dti:1.4-3.5',\n",
    "'dti:3.5-7.7',\n",
    "'dti:7.7-10.5',\n",
    "'dti:10.5-16.1',\n",
    "'dti:16.1-20.3',\n",
    "'dti:20.3-21.7',\n",
    "'dti:21.7-22.4',\n",
    "'dti:22.4-35',\n",
    "'dti:>35',\n",
    "'mths_since_last_delinq:Missing',\n",
    "'mths_since_last_delinq:0-3',\n",
    "'mths_since_last_delinq:4-30',\n",
    "'mths_since_last_delinq:31-56',\n",
    "'mths_since_last_delinq:>=57',\n",
    "'mths_since_last_record:Missing',\n",
    "'mths_since_last_record:0-2',\n",
    "'mths_since_last_record:3-20',\n",
    "'mths_since_last_record:21-31',\n",
    "'mths_since_last_record:32-80',\n",
    "'mths_since_last_record:81-86',\n",
    "'mths_since_last_record:>=86',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here, in the list below, we keep the variable names for the reference categories,\n",
    "# only for the variables we used in our final PD model.\n",
    "ref_categories = ['grade:G',\n",
    "'home_ownership:RENT_OTHER_NONE_ANY',\n",
    "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
    "'verification_status:Verified',\n",
    "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
    "'initial_list_status:f',\n",
    "'term:60',\n",
    "'emp_length:0',\n",
    "'mths_since_issue_d:>84',\n",
    "'int_rate:>20.281',\n",
    "'mths_since_earliest_cr_line:<140',\n",
    "'inq_last_6mths:>6',\n",
    "'acc_now_delinq:0',\n",
    "'annual_inc:<20K',\n",
    "'dti:>35',\n",
    "'mths_since_last_delinq:0-3',\n",
    "'mths_since_last_record:0-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = inputs_test_with_ref_cat.drop(ref_categories, axis = 1)\n",
    "inputs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = reg2.model.predict(inputs_test)\n",
    "# Calculates the predicted values for the dependent variable (targets)\n",
    "# based on the values of the independent variables (inputs) supplied as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test\n",
    "# This is an array of predicted discrete classess (in this case, 0s and 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba = reg2.model.predict_proba(inputs_test)\n",
    "# Calculates the predicted probability values for the dependent variable (targets)\n",
    "# based on the values of the independent variables (inputs) supplied as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba\n",
    "# This is an array of arrays of predicted class probabilities for all classes.\n",
    "# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n",
    "# and the second value is the probability for the observation to belong to the first class, i.e. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba[:][:,1]\n",
    "# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n",
    "# that is, the second element.\n",
    "# In other words, we take only the probabilities for being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba = y_hat_test_proba[: ][: , 1]\n",
    "# We store these probabilities in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba\n",
    "# This variable contains an array of probabilities of being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_test_temp = loan_data_targets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_test_temp.reset_index(drop = True, inplace = True)\n",
    "# We reset the index of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs = pd.concat([loan_data_targets_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)\n",
    "# Concatenates two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.columns = ['loan_data_targets_test', 'y_hat_test_proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.index = loan_data_inputs_test.index\n",
    "# Makes the index of one dataframe equal to the index of another dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Area under the Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 0.9\n",
    "# We create a new column with an indicator,\n",
    "# where every observation that has predicted probability greater than the threshold has a value of 1,\n",
    "# and every observation that has predicted probability lower than the threshold has a value of 0.\n",
    "df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])\n",
    "# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n",
    "# This table is known as a Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]\n",
    "# Here we divide each value of the table by the total number of observations,\n",
    "# thus getting percentages, or, rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]\n",
    "# Here we calculate Accuracy of the model, which is the sum of the diagonal rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
    "# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n",
    "# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
    "# Here we store each of the three arrays in a separate variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n",
    "# thus plotting the ROC curve.\n",
    "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
    "# We plot a seconary diagonal line, with dashed line style and black color.\n",
    "plt.xlabel('False positive rate')\n",
    "# We name the x-axis \"False positive rate\".\n",
    "plt.ylabel('True positive rate')\n",
    "# We name the x-axis \"True positive rate\".\n",
    "plt.title('ROC curve')\n",
    "# We name the graph \"ROC curve\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC = roc_auc_score(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
    "# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n",
    "# from a set of actual values and their predicted probabilities.\n",
    "AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini and Kolmogorov-Smirnov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs = df_actual_predicted_probs.sort_values('y_hat_test_proba')\n",
    "# Sorts a dataframe by the values of a specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs = df_actual_predicted_probs.reset_index()\n",
    "# We reset the index of a dataframe and overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs['Cumulative N Population'] = df_actual_predicted_probs.index + 1\n",
    "# We calculate the cumulative number of all observations.\n",
    "# We use the new index for that. Since indexing in ython starts from 0, we add 1 to each index.\n",
    "df_actual_predicted_probs['Cumulative N Good'] = df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n",
    "# We calculate cumulative number of 'good', which is the cumulative sum of the column with actual observations.\n",
    "df_actual_predicted_probs['Cumulative N Bad'] = df_actual_predicted_probs['Cumulative N Population'] - df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n",
    "# We calculate cumulative number of 'bad', which is\n",
    "# the difference between the cumulative number of all observations and cumulative number of 'good' for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs['Cumulative Perc Population'] = df_actual_predicted_probs['Cumulative N Population'] / (df_actual_predicted_probs.shape[0])\n",
    "# We calculate the cumulative percentage of all observations.\n",
    "df_actual_predicted_probs['Cumulative Perc Good'] = df_actual_predicted_probs['Cumulative N Good'] / df_actual_predicted_probs['loan_data_targets_test'].sum()\n",
    "# We calculate cumulative percentage of 'good'.\n",
    "df_actual_predicted_probs['Cumulative Perc Bad'] = df_actual_predicted_probs['Cumulative N Bad'] / (df_actual_predicted_probs.shape[0] - df_actual_predicted_probs['loan_data_targets_test'].sum())\n",
    "# We calculate the cumulative percentage of 'bad'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Gini\n",
    "plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Bad'])\n",
    "# We plot the cumulative percentage of all along the x-axis and the cumulative percentage 'good' along the y-axis,\n",
    "# thus plotting the Gini curve.\n",
    "plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Population'], linestyle = '--', color = 'k')\n",
    "# We plot a seconary diagonal line, with dashed line style and black color.\n",
    "plt.xlabel('Cumulative % Population')\n",
    "# We name the x-axis \"Cumulative % Population\".\n",
    "plt.ylabel('Cumulative % Bad')\n",
    "# We name the y-axis \"Cumulative % Bad\".\n",
    "plt.title('Gini')\n",
    "# We name the graph \"Gini\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gini = AUROC * 2 - 1\n",
    "# Here we calculate Gini from AUROC.\n",
    "Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KS\n",
    "plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Bad'], color = 'r')\n",
    "# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'bad' along the y-axis,\n",
    "# colored in red.\n",
    "plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Good'], color = 'b')\n",
    "# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'good' along the y-axis,\n",
    "# colored in red.\n",
    "plt.xlabel('Estimated Probability for being Good')\n",
    "# We name the x-axis \"Estimated Probability for being Good\".\n",
    "plt.ylabel('Cumulative %')\n",
    "# We name the y-axis \"Cumulative %\".\n",
    "plt.title('Kolmogorov-Smirnov')\n",
    "# We name the graph \"Kolmogorov-Smirnov\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KS = max(df_actual_predicted_probs['Cumulative Perc Bad'] - df_actual_predicted_probs['Cumulative Perc Good'])\n",
    "# We calculate KS from the data. It is the maximum of the difference between the cumulative percentage of 'bad'\n",
    "# and the cumulative percentage of 'good'.\n",
    "KS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
